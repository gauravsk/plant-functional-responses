---
title: "ML estimates of demographic parameters"
author: "Gaurav Kandlikar"
date: "February 28, 2018"
output: md_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE); thinkpad = FALSE
```

## Set up competition models
First we import the model functions. See `comp_coef_models.R` for the model functions. 

```{r source-functions}
source("code/functions/comp_coef_models.R")
```


## Read in and reformat the seed production data

```{r import-data}
library(dplyr)
library(ggplot2); theme_set(theme_minimal())
if(thinkpad){
  compdens <- read.csv("~/grad/dissertation-ch1/data/performance/calculated/NCompetitors_per_plot.csv")
  seedprod <- read.csv("~/grad/dissertation-ch1/data/performance/seed_production_processed.csv", stringsAsFactors = F)

} else {
  compdens <- read.csv("~/Dropbox/spatial_tapioca/data/performance/calculated/NCompetitors_per_plot.csv")
  seedprod <- read.csv("~/Dropbox/spatial_tapioca/data/performance/seed_production_processed.csv", stringsAsFactors = F)
}
# replace 0s with NAs...
seedprod[which(seedprod$num_seeds_produced == 0), "num_seeds_produced"] <- NA
# maybe switch to be the minimum for the species?
seedprod <- na.omit(seedprod)
seedprod$sp_code <- as.character(seedprod$sp_code)
seedprod$sp_code[which(seedprod$sp_code=="AGSP")] <- "AGHE"
seedprod$sp_code[which(seedprod$sp_code=="CEMI")] <- "CEME"
seedprod$sp_code[which(seedprod$sp_code=="VUMI")] <- "VUMA"

compdens <- dplyr::select(compdens, -N_per_compPlot)
compdens$N_per_neighborhood <- floor(compdens$N_per_neighborhood)
seedprod <- left_join(seedprod, compdens)
# Reset the competitor density in Lambda plots to 0
seedprod[which(seedprod$plot_type == "L"), "N_per_neighborhood"] <- 0

seedprod <- seedprod %>% mutate(num_seeds_produced = as.integer(num_seeds_produced),
                    N_per_neighborhood = as.integer(N_per_neighborhood))

seedprod <- seedprod %>% group_by(sp_code, plot_num) %>% filter(n() > 3) %>% ungroup

# Fit the four models to just AGHE right now
# Later we loop over all the species in the dataset

# Set up empty matrices to be filled during the for-loop --------------

# Model 1
mod1pars <- matrix(NA, ncol = 3, nrow = length(unique(seedprod$sp_code)))
rownames(mod1pars) <- unique(seedprod$sp_code)
colnames(mod1pars) <- c("lambda", "sigma", "convergence_code")

# Model 2
mod2pars <- matrix(NA, ncol = 4, nrow = length(unique(seedprod$sp_code)))
rownames(mod2pars) <- unique(seedprod$sp_code)
colnames(mod2pars) <- c("lambda", "alpha", "sigma", "convergence_code")

# Model 3
mod3pars <- matrix(NA, ncol = 28, nrow = length(unique(seedprod$sp_code)))
rownames(mod3pars) <- unique(seedprod$sp_code)
colnames(mod3pars) <- c("lambda", "alpha", "sigma", paste0("plot_",740:763, "_effect"), "convergence_code")

# Model 4
mod4pars <- matrix(NA, ncol = 27, nrow = length(unique(seedprod$sp_code)))
rownames(mod4pars) <- unique(seedprod$sp_code)
colnames(mod4pars) <- c("alpha", "sigma", paste0("plot_",740:763, "_lambda"), "convergence_code")

# Model 5
mod5pars <- matrix(NA, ncol = 50, nrow = length(unique(seedprod$sp_code)))
rownames(mod5pars) <- unique(seedprod$sp_code)
colnames(mod5pars) <- c("sigma", paste0("plot_",740:763, "_alpha"),paste0("plot_",740:763, "_lambda"), "convergence_code")
```

## And now we run the models on all species

```{r run-models}
source("code/functions/make_site_matrices.R")
for(sp in unique(seedprod$sp_code)) {
  # Format vectors for the current species ===========
  lseed <- seedprod %>% filter(sp_code == sp) %>% dplyr::select(num_seeds_produced) %>% log %>% unlist %>% as.numeric
  dens <- seedprod %>% filter(sp_code == sp) %>% dplyr::select(N_per_neighborhood) %>% unlist %>% as.numeric
  densvector <- seedprod %>% filter(sp_code == sp) %>% group_by(plot_num) %>% filter(n() > 3) %>% ungroup() %>% dplyr::select(N_per_neighborhood) %>% unlist %>% as.numeric
  which_plot <-  seedprod %>% filter(sp_code == sp) %>% dplyr::select(plot_num) %>% group_by(plot_num) %>% filter(n() > 3) %>% unlist %>% as.numeric


  # Make a site matrix- to be used by models 3, 4, and 5 =====
  site_matrix <- seedprod %>% filter(sp_code == sp) %>% make_site_matrix() %>% dplyr::select(site1:site24) %>% as.matrix
  
  # Make a competition density matrix- to be used for Model 5 ======
  compdens <- seedprod %>% filter(sp_code == sp) %>% make_compdens_matrix() %>% dplyr::select(site1:site24) %>% as.matrix
  
  # If there's fewer than 4 measured plants at a site, set it to NA later.
  set_to_na <- as.vector(which(colSums(site_matrix) < 4))
  # If there's fewer than 2 measured plants in competition at a site, set the alpha estimate to NA later
  num_comp_individuals <- seedprod %>% filter(sp_code == sp, plot_type == "C") %>% make_site_matrix() %>% dplyr::select(site1:site24) %>% colSums %>% as.numeric
  set_alphas_to_na <- which(num_comp_individuals < 2)
  if(length(set_alphas_to_na) > 0) {
    which_plots_have_alpha <- unique(seedprod$plot_num)[-set_alphas_to_na]
  } else {
    which_plots_have_alpha <- unique(seedprod$plot_num)
  }
  # The model runs start here --------------
  
  # Model 1 ======
  # Set the mean and SD of lseed as the starting points for the first optimization
  par = c(mean(lseed), sd(lseed))
  # cat("Mean and SD for species:\n"); par
  for(i in 1:25){
    testcomp1<-optim(par,compmodel1)
    par<-testcomp1$par
    if(testcomp1$convergence==0){break}
  }
  print(paste(sp, ": Model 1 converged on rep", i))
  mod1pars[sp,] <- c(testcomp1$par, testcomp1$convergence)
  # Predicted = ...
  preds <- rep(mod1pars[sp,"lambda"], length(lseed))
  mod1_predictions <- data.frame(observed = exp(lseed),
                                 dens = densvector,
                                 predicted = preds)
  
  gg_mod1_predVobs <- ggplot(mod1_predictions) + geom_point(aes(x = observed, y = predicted))

  # Model 2 ========
  # Set mean and SD from model 1, and alpha as 0.001 to run model 2
  # Model 2 expects parameters in the order c(lambda, alpha, sigma)
  par<-c(testcomp1$par[1],0.001,testcomp1$par[2])
  for(i in 1:25){
    testcomp2<-optim(par, compmodel2, lower = c(1, 0.0000001, 0.00000001), method="L-BFGS-B")
    par<-testcomp2$par
    if(testcomp2$convergence==0){break}
  }
  print(paste(sp, ": Model 2 converged on rep", i))
  mod2pars[sp,] <- c(testcomp2$par, testcomp2$convergence)
  # Predicted = ...
  preds <- mod2pars[sp,"lambda"]/(1+mod2pars[sp,"alpha"]*densvector)
  mod2_predictions <- data.frame(observed = exp(lseed),
                                 dens = densvector,
                                 predicted = preds)
  gg_mod2_predVobs <- ggplot(mod2_predictions) + geom_point(aes(x = observed, y = predicted))


  # Model 3 ========
  # Set the estimated global lambda, sigma, and global alpha as the beginning pars for this model; 
  # The site-specific modifiers of lambda are initialized at 0.01
  # Model 3 expects parameters in the order c(lambda,alpha, sigma, site effects*24)
  par <- c(testcomp2$par[1],testcomp2$par[2], testcomp2$par[3], rep(0.01, 24))
  for(i in 1:50){
    testcomp3<-optim(par, compmodel3, method="L-BFGS-B", lower=c(1,0,0.000001, rep(0, 24)),
                     control=list(maxit=10000), hessian=TRUE)
    par<-testcomp3$par
    if(testcomp3$convergence==0){break}
  }
  print(paste(sp, ": Model 3 converged on rep", i))
  # testcomp3$par[set_to_na+3] <- 0
  pars_to_save <- testcomp3$par
  pars_to_save[set_to_na+3] <- NA
  # pars_to_save[set_alphas_to_na+3] <- NA
  mod3pars[sp,] <- c(pars_to_save, testcomp3$convergence)
  # Predicted = ...
  preds <- ((mod3pars[sp,"lambda"] + mod3pars[sp, 4:27]) %>% rep(., colSums(site_matrix)))/(1 + (mod3pars[sp,"alpha"]*densvector))
  
  mod3_predictions <- data.frame(observed = exp(lseed),
                                 dens = densvector,
                                 predicted = preds)

  gg_mod3_predVobs <- ggplot(mod3_predictions) + geom_point(aes(x = observed, y = predicted))

  # Model 4 =======
  # Set the estimated global lambda as the initial value of the site specific lambdas;
  # The sigma and alphas as estimated in model 2 as well.
  # Model 4 expects parameters in the order c(alpha, sigma, lambdas*24)
  par <- c(testcomp2$par[2], testcomp2$par[3], rep(testcomp2$par[1], 24))
  for(i in 1:50){
    testcomp4<-optim(par,compmodel4,  method="L-BFGS-B", lower=c(0,0.0000000001, c(rep(1,24))),
                     control=list(maxit=10000), hessian=TRUE)
    par<-testcomp4$par
    if(testcomp4$convergence==0){break}
  }
  print(paste(sp, ": Model 4 converged on rep", i))
  pars_to_save <- testcomp4$par
  pars_to_save[set_to_na+2] <- NA
  # pars_to_save[set_alphas_to_na+2] <- NA

  mod4pars[sp,] <- c(pars_to_save, testcomp4$convergence)
  
  preds <- rep(mod4pars[sp, 3:26], colSums(site_matrix))/(1 + mod4pars[sp,"alpha"]*densvector)
  mod4_predictions <- data.frame(observed = exp(lseed),
                                 dens = densvector,
                                 predicted = preds)
  gg_mod4_predVobs <- ggplot(mod4_predictions) + geom_point(aes(x = observed, y = predicted)) 

  # Model 5 =========
  # Set the parameters based on the output from Model 4- sigma is mod4[2], testcomp[1] is the alpha est, 
  # and 3:26 are the site-specific lambda estimates
  # Model 5 expects parameters in the order c(sigma, alphas*24, lambdas*24)
  par <- c(testcomp4$par[2], rep(testcomp4$par[1], 24), testcomp4$par[3:26])
  for(i in 1:200){
    testcomp5<-optim(par,compmodel5,  method="L-BFGS-B", lower=c(0.0000001, rep(0.0000000001, 24), rep(1, 24)),
                     control=list(maxit=10000), hessian=TRUE)
    par<-testcomp5$par
    if(testcomp5$convergence==0){break}
  }
  print(paste(sp, ": Model 5 converged on rep", i))
  pars_to_save <- testcomp5$par
  pars_to_save[set_to_na+1] <- NA
  pars_to_save[set_alphas_to_na+1] <- NA
  pars_to_save[set_to_na+25] <- NA

  mod5pars[sp,] <- c(pars_to_save, testcomp5$convergence)
  
  preds =  rep(mod5pars[sp, 26:49], colSums(site_matrix))/(1+(rep(mod5pars[sp,2:25], colSums(site_matrix))*densvector))
  mod5_predictions <- data.frame(observed = exp(lseed),
                                 dens = densvector,
                                 predicted = preds,
                                 plot = which_plot)
  gg_mod5_predVobs <- ggplot(mod5_predictions) + 
    geom_point(aes(x = observed, y = predicted), color = alpha("black", .66), size = 2) + 
    ggtitle(paste0(sp,": Predicted vs. Observed (site specific L & alpha)"))
  
  # PREDICTED SEED OUTPUT AGAINST BACKGROUND DENSITY
  tmpx<-seq(0,50,length=200)
  predicted_suppression <- data.frame(plot = rep(unique(which_plot[(which_plot %in% which_plots_have_alpha)]), each = length(tmpx)),
                                      density = rep(tmpx, length(unique(which_plot[(which_plot %in% which_plots_have_alpha)]))),
                                      predicted_seed = na.omit(rep(mod5pars[sp, 26:49], each = 200)/(1+(rep(mod5pars[sp,2:25], each = 200)*tmpx))))
  
  gg_mod5_predVdens <- ggplot(mod5_predictions) + geom_point(aes(x = dens, y = observed), 
                                                             color = alpha("black", .75), size = 2) + 
    geom_line(data = predicted_suppression, aes(x = density,y = predicted_seed)) + 
    facet_wrap(~plot, scales = "free_y") + 
    theme(strip.background = element_blank(), strip.placement = "outside") + 
    ggtitle("Seed production across densities")
  # Plot the seed production
  # Make a vector of the predicted seed output- this is kind of tricky given the spatial goings on
  # Predicted seed output should be (Lambda_for_plot)/(1+alpha_for_plot*plot_density)
  # but we need things to repeat the right number of times...
  plot_to_save_1 <- cowplot::plot_grid(gg_mod5_predVobs, gg_mod5_predVdens, rel_widths = c(2,3))
  plot_to_save_2 <- cowplot::plot_grid(gg_mod1_predVobs, gg_mod2_predVobs, gg_mod3_predVobs, gg_mod4_predVobs)

  ggsave(plot_to_save_2, filename = paste0("figs/scripted_figs/compmodels1-4_performance/mods1-4_performance_",sp,".png"), width = 13, height = 7)
  ggsave(plot_to_save_1, filename = paste0("figs/scripted_figs/compmodel5_peformance/mod5_performance_",sp,".png"), width = 13, height = 7)
  
  # Compute AICs and write out AIC table =========
  sampsize = length(lseed)
  
  npar <- length(testcomp1$par); AIC1 <- 2*(testcomp1$value+npar); AICc1 <- AIC1+(2*npar*(npar+1))/(sampsize-npar-1) 
  npar <- length(testcomp2$par); AIC2 <- 2*(testcomp2$value+npar); AICc2 <- AIC2+(2*npar*(npar+1))/(sampsize-npar-1)
  npar <- length(testcomp3$par); AIC3 <- 2*(testcomp3$value+npar); AICc3 <- AIC3+(2*npar*(npar+1))/(sampsize-npar-1)
  npar <- length(testcomp4$par); AIC4 <- 2*(testcomp4$value+npar); AICc4 <- AIC4+(2*npar*(npar+1))/(sampsize-npar-1)
  npar <- length(testcomp5$par); AIC5 <- 2*(testcomp5$value+npar); AICc5 <- AIC5+(2*npar*(npar+1))/(sampsize-npar-1)
  
  aics_to_print <- cbind(species = rep(sp, 5), model = paste("Model",1:5), aic = c(AIC1, AIC2, AIC3, AIC4, AIC5), aic_corrected = c(AICc1, AICc2, AICc3, AICc4, AICc5))
  write.csv(aics_to_print, file = paste0("model_outputs/", sp,"_aics.csv"), quote = F)
}

# Save the outputs ===========
# NOTE! Double check these outputs. Why are the convergence codes 1 now???
write.csv(mod1pars, "model_outputs/mod1pars.csv", quote = F)
write.csv(mod2pars, "model_outputs/mod2pars.csv", quote = F)
write.csv(mod3pars, "model_outputs/mod3pars.csv", quote = F)
write.csv(mod4pars, "model_outputs/mod4pars.csv", quote = F)
write.csv(mod5pars, "model_outputs/mod5pars.csv", quote = F)
```

## To Do

Here's some things that are yet to be done:   

 - [ ] Figure out exactly how to use the Hessian that is produced from testcomps3,4,5. I have a vague understanding that the square root of the diagonals of the inverse of the matrix represent the standard error of the ML estimate.   
 - [x] Add code to plot the observed seed output vs. the seed output expected from the models  
 - [ ] Compare the output from these models with a second analysis, of estimating all of these parameters from a call to `lmer` with `species*site*competitor density` as the predictor.
 - [x] Up the max replicates of model 5 from 100 to 200, to see if the few remaining species that don't converge eventually do (maybe this doesn't matter in the end, can't hurt to try.)  
 - [ ] Filter out individual parameter estimates based on <= 4 data points, not just 4 points per site (i.e. sometimes, alpha is estimated when there was only 1 comp individual was measured). See how much data is lost this way.